# -*- coding: utf-8 -*-
"""Model_Summarize.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hWkT-iChjBuFQPyWUtlUHO1V8yPZqqcV
"""



import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import gensim.downloader
from gensim.parsing.preprocessing import remove_stopwords
import nltk
from nltk.tokenize import word_tokenize

#Models
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartTokenizer, BartForConditionalGeneration, AutoModel
from sentence_transformers import SentenceTransformer
import torch
import numpy as np
import os

#kmeans clustering
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist, cosine

nltk.download('punkt')

class TextSummarizer:
    def __init__(self, model_name, embedding_model_name, max_length=512, overlap=80, num_clusters=3):
        self.model_name = model_name
        self.embedding_model = gensim.downloader.load(embedding_model_name)
        self.max_length = max_length
        self.overlap = overlap
        self.num_clusters = num_clusters
        self.model = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def split_context(self, context):
        chunks = []
        start = 0
        while start < len(context):
            end = min(start + self.max_length, len(context))
            chunks.append(context[start:end])
            if end == len(context):
                break
            start = end - self.overlap
        return chunks

    def vectorize_text(self, text):
        words = word_tokenize(remove_stopwords(text.lower()))
        words = [word for word in words if word in self.embedding_model.key_to_index]
        if words:
            text_vector = np.mean([self.embedding_model[word] for word in words], axis=0)
        else:
            text_vector = np.zeros(self.embedding_model.vector_size)
        return text_vector

    def summarize(self, context, question):
        vectorized_question = self.vectorize_text(question)
        vectorized_question = np.array(vectorized_question).reshape(1, -1)

        chunks = self.split_context(context)
        vectorized_chunks = [self.vectorize_text(chunk) for chunk in chunks]

        if self.num_clusters > 1:
            additional_centers = np.random.rand(self.num_clusters - 1, vectorized_question.shape[1])
            initial_centers = np.vstack([vectorized_question, additional_centers])
        else:
            initial_centers = vectorized_question

        kmeans = KMeans(n_clusters=self.num_clusters, init=initial_centers, n_init=1, random_state=42)
        kmeans.fit(np.array(vectorized_chunks))

        centroids = kmeans.cluster_centers_
        closest_chunks = []
        for i in range(self.num_clusters):
            distances = [cosine_similarity([centroids[i]], [chunk]) for chunk in vectorized_chunks]
            closest_chunk_idx = np.argmin(distances)
            closest_chunks.append(chunks[closest_chunk_idx])

        closest_chunks.sort(key=lambda chunk: context.index(chunk))
        summary = ' '.join(closest_chunks)
        return summary

    def save_model(self, directory="."):
      # This sets the directory to the current working directory
      # '.' refers to the current directory
      # Create directory if it doesn't exist
      if not os.path.exists(directory):
        os.makedirs(directory)

      # Save the BERT model and tokenizer
      self.model.save_pretrained(directory)
      self.tokenizer.save_pretrained(directory)

      # Save additional attributes as a JSON file if needed
      # For example, save max_length, overlap, num_clusters

      import json
      with open(os.path.join(directory, "config.json"), "w") as config_file:
        json.dump({
            "max_length": self.max_length,
            "overlap": self.overlap,
            "num_clusters": self.num_clusters
            }, config_file)

