{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file(file_path):\n",
    "    \"\"\"Read the log file and return its content.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.readlines()\n",
    "    return content\n",
    "\n",
    "def chunk_text(lines, max_length=512):\n",
    "    \"\"\"Split text into chunks.\"\"\"\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for line in lines:\n",
    "        if len(chunk) + len(line) > max_length:\n",
    "            chunks.append(chunk)\n",
    "            chunk = line\n",
    "        else:\n",
    "            chunk += line\n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "file_path = 'data/log1.out'\n",
    "log_lines = read_log_file(file_path)\n",
    "\n",
    "# Assuming a transformer model with a token limit of 512,\n",
    "# we need to chunk the log data\n",
    "log_chunks = chunk_text(log_lines, max_length=512)\n",
    "\n",
    "# Now, log_chunks contains the log data split into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "\n",
    "def is_text_redundant(text):\n",
    "    \"\"\"\n",
    "    Check if a text is redundant.\n",
    "\n",
    "    A simple approach to check redundancy is to look for repeated phrases or sentences.\n",
    "    This function uses a basic method where it checks for repeated sequences of words.\n",
    "    \n",
    "    param text: The text to be checked for redundancy.\n",
    "    return: True if the text is redundant, False otherwise.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    for i in range(len(words) - 5):  # Check sequences of 5 words\n",
    "        sequence = ' '.join(words[i:i+5])\n",
    "        if sequence in seen:\n",
    "            return True\n",
    "        seen.add(sequence)\n",
    "    return False\n",
    "\n",
    "def determine_overlap(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Determine the best overlapping parameter for splitting a text.\n",
    "\n",
    "    The best overlap is determined based on the length and redundancy of the text.\n",
    "    Shorter and less redundant texts can have smaller overlaps, while longer and more redundant\n",
    "    texts may require larger overlaps to ensure continuity.\n",
    "\n",
    "    param text: The text for which to determine the best overlap.\n",
    "    param max_length: The maximum length of each chunk in characters.\n",
    "    return: The recommended overlap size in characters.\n",
    "    \"\"\"\n",
    "    length = len(text)\n",
    "\n",
    "    # Set base overlap sizes\n",
    "    short_text_overlap = 20  # For short, non-redundant texts\n",
    "    long_text_overlap = 50  # For long or redundant texts\n",
    "\n",
    "    if length < max_length:\n",
    "        return short_text_overlap\n",
    "    else:\n",
    "        if is_text_redundant(text):\n",
    "            # If the text is redundant, increase the overlap to handle complexity\n",
    "            return long_text_overlap + 30  # Increasing overlap for redundancy\n",
    "        else:\n",
    "            # For longer texts which are not redundant\n",
    "            return long_text_overlap\n",
    "\n",
    "def split_context(context, max_length=512):\n",
    "    \"\"\"\n",
    "    Function for splitting context into overlapping chunks.\n",
    "    \n",
    "    param context: This is the text that you want to split into chunks. \n",
    "    The function will split this text based on the max_length and overlap parameters.\n",
    "\n",
    "    param overlap (default=50): This is the number of characters that will overlap between each chunk. \n",
    "    This is used to ensure that the context is not cut off in the middle of a sentence, which could make the text difficult to understand.\n",
    "\n",
    "    param max_length (default=512): This is the maximum length of each chunk. \n",
    "    The function will split the context into chunks of this length, with the exception of the last chunk, which may be shorter.\n",
    "\n",
    "    The function returns a list of chunks, where each chunk is a string of text from the context. \n",
    "    The chunks are created by starting at the beginning of the context and moving forward max_length\n",
    "    characters at a time, with an overlap of overlap characters between each chunk.\n",
    "    \"\"\"\n",
    "    overlap = determine_overlap(context, max_length)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(context):\n",
    "        end = min(start + max_length, len(context))\n",
    "        chunks.append(context[start:end])\n",
    "        if end == len(context):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def answer_question(model, tokenizer, context, question):\n",
    "    \"\"\"\n",
    "    The function answers questions given context and question.\n",
    "    \n",
    "    model: This is the model that you're using to generate answers to the questions. \n",
    "    It could be any model that's capable of question answering, such as a transformer model.\n",
    "\n",
    "    param  tokenizer: This is the tokenizer that corresponds to your model. \n",
    "    It's used to convert your text data into a format that the model can understand.\n",
    "\n",
    "    param context: This is the text that the model will look at to find an answer to the question.\n",
    "\n",
    "    param question: This is the question that you're asking the model. \n",
    "    The model will generate an answer to this question based on the context.\n",
    "\n",
    "    The function returns an answer to the question based on the context. \n",
    "    The answer is generated by finding the tokens with the highest start and end scores, \n",
    "    and joining them together. If the end score is higher than the start score, \n",
    "    they are swapped to ensure the answer makes sense.\n",
    "    \"\"\"\n",
    "    # Encode the context and question\n",
    "    encoded = tokenizer.encode_plus(question, context, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Get the start and end scores for all tokens\n",
    "    result = model(**encoded)\n",
    "    start_scores = result[\"start_logits\"]\n",
    "    end_scores = result[\"end_logits\"]\n",
    "\n",
    "    # Find the tokens with the highest start and end scores\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # If the end score is higher than the start score, swap them\n",
    "    if answer_end < answer_start:\n",
    "        answer_start, answer_end = answer_end, answer_start\n",
    "\n",
    "    # Get the tokens for the answer\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    answer = ' '.join(all_tokens[answer_start : answer_end+1])\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def vectorize_text(model, tokenizer, input_string):\n",
    "    \"\"\"\n",
    "    Vectorize a given input string.\n",
    "    \n",
    "    param model: This is the model used to encode the input string and get the output. \n",
    "    It could be any model that's capable of encoding text, such as a transformer model.\n",
    "\n",
    "    param tokenizer: This is the tokenizer that corresponds to your model. \n",
    "    It's used to convert your text data into a format that the model can understand.\n",
    "\n",
    "    param input_string: This is the text that you want to vectorize. \n",
    "    The function will convert this text into a numerical representation that \n",
    "    can be processed by the machine learning model.\n",
    "\n",
    "    The function returns a vector representation of the input string. \n",
    "    This vector is obtained by averaging the embeddings from the last hidden \n",
    "    state of the model's output.\n",
    "    \"\"\"\n",
    "    # Encode the input string\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_string,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Get the output from the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings\n",
    "    vector = torch.mean(embeddings, dim=1)\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    vector = vector.detach().numpy()\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def calculate_similarity(question_vector, answer_vector):\n",
    "    \"\"\"Calculate the cosine similarity between the question and answer vectors.\n",
    "    \n",
    "    param question_vector: This is the vector representation of the question. \n",
    "    It's obtained by transforming the question text into numerical data that \n",
    "    can be processed by the machine learning model.\n",
    "\n",
    "    param answer_vector: This is the vector representation of the answer. \n",
    "    It's obtained by transforming the answer text into numerical data that \n",
    "    can be processed by the machine learning model.\n",
    "\n",
    "    The function calculates and returns the cosine similarity between the \n",
    "    question and answer vectors. Cosine similarity is a measure of similarity \n",
    "    between two non-zero vectors of an inner product space that measures the \n",
    "    cosine of the angle between them. The closer the cosine similarity to 1, \n",
    "    the more similar the question and answer are.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity = 1 - cosine(question_vector[0], answer_vector[0])\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def find_best_answer(model, tokenizer, context, question, model_vec, num_answers=3, overlap=50, max_length=512):\n",
    "    \"\"\"Find the best answers to the question given a long context\n",
    "    param model: This is the model that you're using to generate answers to the questions. \n",
    "    It could be any model that's capable of question answering, such as a transformer model.\n",
    "\n",
    "    param tokenizer: This is the tokenizer that corresponds to your model.\n",
    "    It's used to convert your text data into a format that the model can understand.\n",
    "\n",
    "    param context: This is the text that the model will look at to find an answer to the question. \n",
    "    In this case, it's a long text that's split into chunks.\n",
    "\n",
    "    param question: This is the question that you're asking the model. \n",
    "    The model will generate an answer to this question based on the context.\n",
    "\n",
    "    param model_vec: This is a model used to vectorize the text, \n",
    "    i.e., convert the text into numerical data that can be processed by the machine learning model.\n",
    "\n",
    "    param num_answers (default=3): This is the number of best answers the function will return.\n",
    "\n",
    "    param overlap (default=50): This is the number of overlapping words between \n",
    "    two consecutive chunks when the context is split into chunks.\n",
    "\n",
    "    param max_length (default=512): This is the maximum length of each chunk. \n",
    "    The context is split into chunks of this length.\n",
    "\n",
    "    The function returns a list of tuples, where each tuple contains an answer \n",
    "    and its similarity score. The list is sorted in ascending order of similarity, \n",
    "    so the first element of the list is the answer with the lowest similarity, \n",
    "    and the last element is the answer with the highest similarity.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Vectorize the question\n",
    "    question_vector = vectorize_text(model_vec, tokenizer, question)\n",
    "    \n",
    "    # Initialize the best answers and their similarities to the question\n",
    "    best_answers = [(None, -1) for _ in range(num_answers)]\n",
    "    \n",
    "    # Split the context into chunks\n",
    "    chunks = split_context(context,max_length)\n",
    "    \n",
    "    # Generate an answer for each chunk and update the best answers if necessary\n",
    "    for chunk in chunks:\n",
    "        answer = answer_question(model, tokenizer, chunk, question)\n",
    "        if answer is not None:\n",
    "            answer_vector = vectorize_text(model_vec, tokenizer, answer)\n",
    "            if answer_vector is not None:\n",
    "                similarity = calculate_similarity(question_vector, answer_vector)\n",
    "                # Check if the similarity is higher than the current lowest in best_answers\n",
    "                if similarity > best_answers[0][1]:\n",
    "                    # Replace the lowest\n",
    "                    best_answers[0] = (answer, similarity)\n",
    "                    # Sort the list so the lowest similarity is first\n",
    "                    best_answers = sorted(best_answers, key=lambda x: x[1])\n",
    "    # Return the answers along with their similarities\n",
    "    return best_answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
